#!/usr/bin/env python3
"""
Efficient Reddit Analyzer

OPTIMIZED for rate limiting:
- Only 2-3 API calls per artist (vs 13+ in enhanced)
- Smart sampling instead of complete data collection
- ~80% reduction in API usage
- 15-20 minutes vs 60+ minutes
"""

import requests
import time
import datetime
import json
import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment
project_root = Path(__file__).parent.parent.parent
env_path = project_root / '.env'
load_dotenv(env_path)

USER_AGENT = os.getenv('REDDIT_USER_AGENT', 'script:mb-script:v1.0 (by /u/tapinda)')
OUTPUT_DIR = project_root / "dev stuff" / "data-scripts" / "reddit_results"
OUTPUT_DIR.mkdir(exist_ok=True)

# EFFICIENT configuration
CONFIG = {
    "MIN_SUBSCRIBERS": 50000,
    "RATE_LIMIT_DELAY": 3.5,  # Conservative rate limiting
    "SAMPLE_SIZE": 50,        # Sample 50 posts instead of paginating 600+
    "REQUEST_TIMEOUT": 15,
    "RETRY_DELAY": 10
}

def safe_request(url, headers, params):
    """Conservative API request with error handling."""
    try:
        response = requests.get(url, headers=headers, params=params, timeout=CONFIG["REQUEST_TIMEOUT"])
        
        if response.status_code == 429:  # Rate limited
            print(f"      ‚è≥ Rate limited, waiting {CONFIG['RETRY_DELAY']}s...")
            time.sleep(CONFIG["RETRY_DELAY"])
            return None
        
        response.raise_for_status()
        return response.json()
        
    except Exception as e:
        print(f"      ‚ùå Request failed: {str(e)[:30]}...")
        return None

def find_primary_subreddit_efficient(artist_name):
    """Find primary subreddit with minimal API calls."""
    print(f"üé§ {artist_name}")
    
    # OPTIMIZATION: Only search the most effective query
    artist_clean = artist_name.replace(' ', '').replace(',', '').replace('.', '').replace("'", "").lower()
    
    print(f"  üîç Searching: '{artist_clean}'")
    
    url = "https://www.reddit.com/subreddits/search.json"
    headers = {'User-Agent': USER_AGENT}
    params = {'q': artist_clean, 'limit': 20, 'sort': 'relevance'}
    
    data = safe_request(url, headers, params)
    
    if not data or 'data' not in data:
        print(f"  ‚ùå No subreddits found")
        return None
    
    # Find best matching subreddit
    best_candidate = None
    best_score = 0
    
    for item in data['data'].get('children', []):
        subreddit_data = item.get('data', {})
        display_name = subreddit_data.get('display_name', '').lower()
        subscribers = subreddit_data.get('subscribers', 0) or 0
        
        # Quick filters
        if subscribers < CONFIG["MIN_SUBSCRIBERS"]:
            continue
        
        # Simple parody detection
        if any(word in display_name for word in ['type', 'meme', 'joke', 'circlejerk']):
            continue
        
        # Score based on name match and size
        score = 0
        if artist_clean == display_name:
            score = 15  # Exact match
        elif artist_clean in display_name:
            score = 10  # Partial match
        elif any(word in display_name for word in ['fan', 'music', 'official']):
            score = 5   # Fan community indicators
        
        if score > best_score:
            best_candidate = subreddit_data
            best_score = score
    
    if best_candidate:
        subreddit_name = best_candidate.get('display_name')
        subscribers = best_candidate.get('subscribers', 0)
        print(f"  üéØ r/{subreddit_name} ({subscribers:,} subs)")
        return best_candidate
    else:
        print(f"  ‚ùå No qualifying subreddit")
        return None

def sample_activity_efficient(subreddit_name):
    """Sample recent activity with single API call."""
    print(f"    üìä Sampling activity...", end="")
    
    url = f"https://www.reddit.com/r/{subreddit_name}/new.json"
    headers = {'User-Agent': USER_AGENT}
    params = {'limit': CONFIG["SAMPLE_SIZE"]}
    
    data = safe_request(url, headers, params)
    
    if not data or 'data' not in data:
        print(" ‚ùå")
        return 0, 0
    
    current_time = time.time()
    month_ago = current_time - (30 * 24 * 60 * 60)
    
    recent_posts = 0
    total_comments = 0
    
    for post in data['data'].get('children', []):
        post_data = post.get('data', {})
        post_time = post_data.get('created_utc', 0)
        
        if post_time >= month_ago:
            recent_posts += 1
            total_comments += post_data.get('num_comments', 0)
    
    # STATISTICAL EXTRAPOLATION from sample
    if recent_posts > 0:
        # Estimate total monthly activity from sample
        sample_ratio = recent_posts / CONFIG["SAMPLE_SIZE"]
        
        # Conservative extrapolation (sample likely represents 30-50% of activity)
        estimated_posts = max(recent_posts, int(recent_posts / (sample_ratio * 0.4)))
        estimated_comments = max(total_comments, int(total_comments / (sample_ratio * 0.4)))
        
        # Cap estimates to prevent wild extrapolations
        estimated_posts = min(estimated_posts, recent_posts * 8)
        estimated_comments = min(estimated_comments, total_comments * 12)
    else:
        estimated_posts = 0
        estimated_comments = 0
    
    print(f" ‚úÖ ~{estimated_posts} posts, ~{estimated_comments:,} comments")
    return estimated_posts, estimated_comments

def analyze_artist_efficient(artist_name):
    """Complete analysis with only 2 API calls per artist."""
    
    # API Call #1: Find subreddit
    primary_sub = find_primary_subreddit_efficient(artist_name)
    
    if not primary_sub:
        return {
            "artist": artist_name,
            "primary_subreddit": None,
            "subreddit_url": None,
            "subscribers": 0,
            "posts_last_month": 0,
            "comments_last_month": 0,
            "total_activity": 0,
            "popularity_score": 0.0,
            "tier": "‚ùå No Presence",
            "api_calls": 1,
            "methodology": "Efficient sampling"
        }
    
    subreddit_name = primary_sub.get('display_name', '')
    subscribers = primary_sub.get('subscribers', 0)
    
    # API Call #2: Sample activity
    posts, comments = sample_activity_efficient(subreddit_name)
    
    total_activity = posts + comments
    
    # Calculate engagement score
    if subscribers > 0:
        score = round((total_activity / subscribers) * 1000, 2)
    else:
        score = 0.0
    
    # Tier classification
    if score >= 5.0:
        tier = "üî• Viral"
    elif score >= 2.0:
        tier = "‚ö° Popular"
    elif score >= 0.5:
        tier = "üìä Present"
    else:
        tier = "üí§ Minimal"
    
    result = {
        "artist": artist_name,
        "primary_subreddit": subreddit_name,
        "subreddit_url": f"https://reddit.com/r/{subreddit_name}",
        "subscribers": subscribers,
        "posts_last_month": posts,
        "comments_last_month": comments,
        "total_activity": total_activity,
        "popularity_score": score,
        "tier": tier,
        "api_calls": 2,
        "methodology": "Efficient sampling"
    }
    
    print(f"    üìà Score: {score} ({tier})")
    return result

def load_artists():
    """Load artist list from file."""
    artist_file = project_root / "scripts" / "top_100_usa_musicians" / "top_100_usa_musicians_names_only.txt"
    
    with open(artist_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    
    # Extract numbered artist names
    artists = []
    for line in lines:
        if any(char.isdigit() and '. ' in line for char in line):
            if '. ' in line:
                artist = line.split('. ', 1)[1].strip()
                if artist and not any(keyword in artist.lower() for keyword in ['top', '=', 'generated']):
                    artists.append(artist)
    
    return artists[:100]

def run_efficient_analysis():
    """Run efficient analysis - 2 API calls per artist."""
    
    print("üöÄ EFFICIENT REDDIT ANALYZER")
    print("=" * 50)
    print("üìä OPTIMIZATIONS:")
    print("   ‚úÖ 2 API calls per artist (vs 13+ enhanced)")
    print("   ‚úÖ Statistical sampling (vs complete pagination)")
    print("   ‚úÖ Conservative rate limiting")
    print("   ‚úÖ ~80% reduction in API usage")
    print("=" * 50)
    
    artists = load_artists()
    print(f"üìã Analyzing {len(artists)} artists\n")
    
    results = []
    start_time = time.time()
    total_api_calls = 0
    
    for i, artist in enumerate(artists, 1):
        print(f"[{i:3d}/100] {'-' * 20}")
        
        try:
            result = analyze_artist_efficient(artist)
            results.append(result)
            total_api_calls += result.get('api_calls', 2)
            
            # Progress update
            elapsed = time.time() - start_time
            avg_time = elapsed / i
            remaining = avg_time * (len(artists) - i)
            
            print(f"    ‚è±Ô∏è  {i}/100 ({i/len(artists)*100:.1f}%) | {remaining/60:.1f}min remaining")
            
        except Exception as e:
            print(f"    ‚ùå ERROR: {e}")
            results.append({"artist": artist, "error": str(e)})
        
        # Rate limiting between artists
        time.sleep(CONFIG["RATE_LIMIT_DELAY"])
    
    # Save results
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = OUTPUT_DIR / f"efficient_analysis_{timestamp}.json"
    
    successful = [r for r in results if 'error' not in r and r.get('popularity_score', 0) > 0]
    
    final_data = {
        "analysis_date": datetime.datetime.now().isoformat(),
        "methodology": "Efficient sampling - 2 API calls per artist",
        "summary": {
            "total_artists": len(artists),
            "successful_analyses": len([r for r in results if 'error' not in r]),
            "artists_with_presence": len(successful),
            "total_api_calls": total_api_calls,
            "avg_calls_per_artist": total_api_calls / len(artists)
        },
        "results": results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=2, ensure_ascii=False)
    
    elapsed = time.time() - start_time
    print(f"\nüìÑ Analysis complete: {output_file}")
    print(f"‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes")
    print(f"üì° Total API calls: {total_api_calls}")
    print(f"‚úÖ Artists with presence: {len(successful)}")
    
    return results

if __name__ == "__main__":
    print("üéØ Efficient Reddit Analyzer - Ready")
    print("üí° Call run_efficient_analysis() to start") 