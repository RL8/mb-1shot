import praw
import time
import datetime
from collections import defaultdict
import json
import csv
import os

# --- CONFIGURATION ---
REDDIT_CLIENT_ID = "YOUR_CLIENT_ID"  # Replace with your Reddit app client ID
REDDIT_CLIENT_SECRET = "YOUR_CLIENT_SECRET" # Replace with your Reddit app client secret
REDDIT_USER_AGENT = "ArtistFanSubredditFinderAudit/1.0 (by u/YOUR_REDDIT_USERNAME)" # Replace with your Reddit username

# --- TUNABLE PARAMETERS (Snapshot them for audit) ---
CONFIG_PARAMS = {
    "SUBREDDIT_SEARCH_LIMIT": 15,
    "POST_ACTIVITY_CHECK_LIMIT": 10,
    "MIN_SUBSCRIBERS_FOR_CONSIDERATION": 100,
    "MIN_AVG_COMMENTS_PER_POST": 2,
    "MIN_HOT_POST_AGE_HOURS": 24,
    "RATE_LIMIT_DELAY": 1.5,
    "RELEVANCE_WEIGHT": 0.6,
    "ACTIVITY_WEIGHT": 0.4,
    "FINAL_CONFIDENCE_THRESHOLD": 5, # Minimum confidence for main inclusion
    "FINAL_CONFIDENCE_THRESHOLD_SMALL_ACTIVE": 2 # Lower threshold for small but very active subs
}

# --- FILE PATH CONFIGURATION ---
BASE_OUTPUT_DIR = "audit_results" # New directory for audit trail approach

# --- PRAW Initialization ---
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent=REDDIT_USER_AGENT
)

# Ensure base output directory exists
os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

def safe_filename(name):
    """Converts a string into a safe filename."""
    return "".join(c for c in name if c.isalnum() or c in (' ', '.', '_')).rstrip().replace(' ', '_')

def get_current_timestamp():
    """Returns a formatted timestamp string."""
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")

def save_json_audit(data, filename):
    """Saves audit data to a JSON file."""
    filepath = os.path.join(BASE_OUTPUT_DIR, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"  Saved audit file: {filepath}")

def save_csv_summary(data, filename, fieldnames):
    """Saves a list of dictionaries to a CSV file."""
    filepath = os.path.join(BASE_OUTPUT_DIR, filename)
    with open(filepath, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
    print(f"  Saved summary CSV: {filepath}")

def create_subreddit_entry(subreddit_obj, artist_name, query, is_new=True):
    """
    Creates or updates a subreddit entry with initial data and logs its discovery.
    """
    entry = {
        "display_name": subreddit_obj.display_name,
        "subreddit_id": subreddit_obj.id, # Unique identifier
        "artist_name_searched": artist_name,
        "subscribers": subreddit_obj.subscribers,
        "public_description": subreddit_obj.public_description,
        "created_utc_subreddit": subreddit_obj.created_utc,
        "quarantine": subreddit_obj.quarantine,
        "subreddit_type": subreddit_obj.subreddit_type,
        "raw_search_data": [], # To store details of each query hit
        "relevance": {"score": 0, "details": []},
        "activity": {"score": 0, "details": [], "status": "Not Analyzed"},
        "confidence_score": 0,
        "final_decision": {"included_in_final_results": False, "reason": "Not yet processed"},
        "audit_trail_log": [] # Chronological log for this specific subreddit
    }
    
    log_entry = {
        "timestamp": get_current_timestamp(),
        "event": "Subreddit discovered",
        "details": {
            "query_used": query,
            "subscribers_on_discovery": subreddit_obj.subscribers,
            "is_new_entry": is_new
        }
    }
    entry["audit_trail_log"].append(log_entry)
    return entry

def log_event(subreddit_entry, event, details=None):
    """Adds an event to the audit trail log of a specific subreddit entry."""
    log_entry = {
        "timestamp": get_current_timestamp(),
        "event": event,
        "details": details if details is not None else {}
    }
    subreddit_entry["audit_trail_log"].append(log_entry)

def analyze_subreddit_activity(subreddit_entry):
    """
    Analyzes the recent activity of a subreddit and updates its entry.
    """
    subreddit_name = subreddit_entry["display_name"]
    log_event(subreddit_entry, "Activity analysis started")

    try:
        subreddit = reddit.subreddit(subreddit_name)
        
        if subreddit.quarantine:
            subreddit_entry["activity"]["status"] = "Quarantined"
            log_event(subreddit_entry, "Activity analysis skipped", {"reason": "Subreddit is quarantined"})
            return 0, 9999 
        if subreddit.subreddit_type not in ['public', 'restricted']:
            subreddit_entry["activity"]["status"] = f"Not public ({subreddit.subreddit_type})"
            log_event(subreddit_entry, "Activity analysis skipped", {"reason": f"Subreddit type is '{subreddit.subreddit_type}'"})
            return 0, 9999 

        hot_posts = list(subreddit.hot(limit=CONFIG_PARAMS["POST_ACTIVITY_CHECK_LIMIT"]))
        if not hot_posts:
            subreddit_entry["activity"]["status"] = "No hot posts found"
            log_event(subreddit_entry, "Activity analysis finished", {"result": "No hot posts"})
            return 0, 9999 

        total_comments = sum(p.num_comments for p in hot_posts)
        avg_comments = total_comments / len(hot_posts) if hot_posts else 0
        hottest_post_age_hours = (datetime.datetime.utcnow().timestamp() - hot_posts[0].created_utc) / 3600

        subreddit_entry["activity"]["avg_comments_on_hot_posts"] = avg_comments
        subreddit_entry["activity"]["hottest_post_age_hours"] = hottest_post_age_hours
        subreddit_entry["activity"]["status"] = "Success"
        
        activity_score = 0
        activity_details = []

        if avg_comments >= CONFIG_PARAMS["MIN_AVG_COMMENTS_PER_POST"]:
            activity_score += 2 
            activity_details.append(f"Avg comments met threshold ({avg_comments:.1f} >= {CONFIG_PARAMS['MIN_AVG_COMMENTS_PER_POST']})")
            if hottest_post_age_hours < CONFIG_PARAMS["MIN_HOT_POST_AGE_HOURS"]:
                activity_score += 3 
                activity_details.append(f"Hottest post recent ({hottest_post_age_hours:.1f} hrs < {CONFIG_PARAMS['MIN_HOT_POST_AGE_HOURS']} hrs)")
                if hottest_post_age_hours < (CONFIG_PARAMS["MIN_HOT_POST_AGE_HOURS"] / 2):
                    activity_score += 2 
                    activity_details.append(f"Hottest post very recent ({hottest_post_age_hours:.1f} hrs < {CONFIG_PARAMS['MIN_HOT_POST_AGE_HOURS']/2:.1f} hrs)")
        
        if subreddit_entry["subscribers"] > 1000:
            activity_score += 1
            activity_details.append("Subscribers > 1,000")
        if subreddit_entry["subscribers"] > 10000:
            activity_score += 2
            activity_details.append("Subscribers > 10,000")
        if subreddit_entry["subscribers"] > 100000:
            activity_score += 2
            activity_details.append("Subscribers > 100,000")

        subreddit_entry["activity"]["score"] = activity_score
        subreddit_entry["activity"]["details"] = activity_details
        log_event(subreddit_entry, "Activity analysis completed", {"score": activity_score, "details": activity_details})

        return avg_comments, hottest_post_age_hours

    except Exception as e:
        subreddit_entry["activity"]["status"] = f"Error during analysis: {str(e)}"
        log_event(subreddit_entry, "Activity analysis failed", {"error": str(e)})
        return 0, 9999 

def find_artist_subreddits(artist_name):
    """
    Searches for subreddits related to an artist, assesses relevance/activity,
    and maintains a detailed audit trail within each subreddit's data.
    """
    all_potential_subreddits = {} # Key: subreddit_id, Value: detailed_entry_dict
    
    search_queries = [
        artist_name,
        f"r/{artist_name.replace(' ', '').lower()}", 
        f"{artist_name} fans",
        f"{artist_name} music",
        f"{artist_name} band",
        f"r/{artist_name.replace(' ', '_').lower()}", 
        f"r/{artist_name.replace(' ', '-').lower()}"  
    ]
    
    strong_keywords = [
        artist_name.lower(),
        f"r/{artist_name.lower().replace(' ', '')}",
        f"r/{artist_name.lower().replace(' ', '_')}",
        f"r/{artist_name.lower().replace(' ', '-')}",
        "official", "fans", "community", "sub", "music", "band", "artist", "album"
    ]

    print(f"\n--- Searching for subreddits for: {artist_name} ---")

    for query in sorted(list(set(search_queries))): 
        print(f"  Searching with query: '{query}'")
        try:
            for subreddit in reddit.subreddits.search(query=query, limit=CONFIG_PARAMS["SUBREDDIT_SEARCH_LIMIT"]):
                sub_id = subreddit.id
                
                if sub_id not in all_potential_subreddits:
                    sub_entry = create_subreddit_entry(subreddit, artist_name, query, is_new=True)
                    all_potential_subreddits[sub_id] = sub_entry
                else:
                    sub_entry = all_potential_subreddits[sub_id]
                    log_event(sub_entry, "Subreddit re-discovered", {"query_used": query})

                # Add raw search data snippet for traceability
                sub_entry["raw_search_data"].append({
                    "query": query,
                    "timestamp": get_current_timestamp(),
                    "subscribers_at_query_time": subreddit.subscribers, # May differ from final
                    "public_description_at_query_time": subreddit.public_description
                })

                # Calculate initial relevance score
                relevance_score = 0
                relevance_details = []
                lower_display_name = subreddit.display_name.lower()
                lower_description = subreddit.public_description.lower() if subreddit.public_description else ""

                if artist_name.lower() in lower_display_name:
                    relevance_score += 3
                    relevance_details.append(f"Artist name '{artist_name.lower()}' in display name")
                    if lower_display_name == artist_name.lower().replace(' ', ''): 
                         relevance_score += 5
                         relevance_details.append("Exact (no space) display name match")
                    elif lower_display_name == f"r/{artist_name.lower().replace(' ', '_')}":
                        relevance_score += 4
                        relevance_details.append("Underscore display name match")

                if any(keyword in lower_display_name for keyword in strong_keywords):
                    relevance_score += 2
                    relevance_details.append("Strong keyword in display name")
                if any(keyword in lower_description for keyword in strong_keywords):
                    relevance_score += 1
                    relevance_details.append("Strong keyword in description")

                if artist_name.lower() == lower_display_name.replace('r/', '').replace('_', '').replace('-', '') or \
                   f"{artist_name.lower()}fans" == lower_display_name.replace('r/', '').replace('_', '').replace('-', ''):
                    relevance_score += 10 
                    relevance_details.append("Very strong indicator (direct name or fan term match)")

                # Update relevance in the main entry
                # We'll take the max relevance if found by multiple queries
                if relevance_score > sub_entry["relevance"]["score"]:
                    sub_entry["relevance"]["score"] = relevance_score
                    sub_entry["relevance"]["details"] = relevance_details
                    log_event(sub_entry, "Relevance score updated", {"new_score": relevance_score, "details": relevance_details})
                
                # Decision to continue processing (based on initial filter)
                if subreddit.subscribers < CONFIG_PARAMS["MIN_SUBSCRIBERS_FOR_CONSIDERATION"] and sub_entry["relevance"]["score"] < 5:
                    log_event(sub_entry, "Initial filter decision", {"reason": "Skipped due to low subscribers and low initial relevance"})
                    sub_entry["final_decision"]["reason"] = "Filtered out early (low subs & low relevance)"
                    continue # Skip to next subreddit, but keep in all_potential_subreddits for audit

            time.sleep(CONFIG_PARAMS["RATE_LIMIT_DELAY"]) 

        except praw.exceptions.APIException as e:
            print(f"  [API Error searching for '{query}']: {e}. Waiting longer...")
            time.sleep(CONFIG_PARAMS["RATE_LIMIT_DELAY"] * 5)
        except Exception as e:
            print(f"  [General Error searching for '{query}']: {e}")
            time.sleep(CONFIG_PARAMS["RATE_LIMIT_DELAY"] * 2)

    # Now, assess activity and make final inclusion decisions
    final_subreddits_for_artist = []
    print(f"\n  Analyzing activity for {len(all_potential_subreddits)} potential subreddits...")
    for sub_id, subreddit_entry in all_potential_subreddits.items():
        if "final_decision" in subreddit_entry and subreddit_entry["final_decision"]["reason"] == "Filtered out early (low subs & low relevance)":
            continue # Don't re-process those already filtered early

        avg_comments, hottest_post_age_hours = analyze_subreddit_activity(subreddit_entry)
        
        # Calculate final confidence score
        confidence_score = (subreddit_entry["relevance"]["score"] * CONFIG_PARAMS["RELEVANCE_WEIGHT"]) + \
                           (subreddit_entry["activity"]["score"] * CONFIG_PARAMS["ACTIVITY_WEIGHT"])
        subreddit_entry["confidence_score"] = confidence_score
        log_event(subreddit_entry, "Final confidence score calculated", {"score": confidence_score})
        
        inclusion_reason = ""
        is_included = False

        if confidence_score > CONFIG_PARAMS["FINAL_CONFIDENCE_THRESHOLD"] and \
           subreddit_entry["subscribers"] >= CONFIG_PARAMS["MIN_SUBSCRIBERS_FOR_CONSIDERATION"]:
            is_included = True
            inclusion_reason = f"Passed main confidence threshold ({confidence_score:.2f} > {CONFIG_PARAMS['FINAL_CONFIDENCE_THRESHOLD']}) and subscriber count met ({subreddit_entry['subscribers']} >= {CONFIG_PARAMS['MIN_SUBSCRIBERS_FOR_CONSIDERATION']})"
        elif confidence_score > CONFIG_PARAMS["FINAL_CONFIDENCE_THRESHOLD_SMALL_ACTIVE"] and \
             subreddit_entry["subscribers"] < CONFIG_PARAMS["MIN_SUBSCRIBERS_FOR_CONSIDERATION"] and \
             subreddit_entry["activity"]["avg_comments_on_hot_posts"] > 0:
            is_included = True
            inclusion_reason = f"Passed small-active threshold ({confidence_score:.2f} > {CONFIG_PARAMS['FINAL_CONFIDENCE_THRESHOLD_SMALL_ACTIVE']}) and active despite low subs ({subreddit_entry['subscribers']} < {CONFIG_PARAMS['MIN_SUBSCRIBERS_FOR_CONSIDERATION']})"
        else:
            inclusion_reason = f"Did not meet inclusion criteria (final score: {confidence_score:.2f}, subs: {subreddit_entry['subscribers']})"

        subreddit_entry["final_decision"]["included_in_final_results"] = is_included
        subreddit_entry["final_decision"]["reason"] = inclusion_reason
        log_event(subreddit_entry, "Final inclusion decision", {"included": is_included, "reason": inclusion_reason})

        if is_included:
            final_subreddits_for_artist.append(subreddit_entry)

        time.sleep(CONFIG_PARAMS["RATE_LIMIT_DELAY"]) 

    final_subreddits_for_artist.sort(key=lambda x: x['confidence_score'], reverse=True)

    # Save the full audit trail for all potential subreddits found for this artist
    save_json_audit(list(all_potential_subreddits.values()), 
                    f"{safe_filename(artist_name)}_subreddits_audit_trail.json")
    
    # Save the final summary for this artist (CSV)
    if final_subreddits_for_artist:
        csv_fieldnames = [
            "display_name", "subscribers", "avg_comments_on_hot_posts", 
            "hottest_post_age_hours", "relevance_score", "activity_score", 
            "confidence_score", "public_description", 
            "final_decision_included", "final_decision_reason" # Include decision in CSV
        ]
        # Flatten the data for CSV
        csv_data = []
        for sub_data in final_subreddits_for_artist:
            row = {
                "display_name": sub_data["display_name"],
                "subscribers": sub_data["subscribers"],
                "avg_comments_on_hot_posts": sub_data["activity"].get("avg_comments_on_hot_posts", 0),
                "hottest_post_age_hours": sub_data["activity"].get("hottest_post_age_hours", 9999),
                "relevance_score": sub_data["relevance"]["score"],
                "activity_score": sub_data["activity"]["score"],
                "confidence_score": sub_data["confidence_score"],
                "public_description": sub_data["public_description"],
                "final_decision_included": sub_data["final_decision"]["included_in_final_results"],
                "final_decision_reason": sub_data["final_decision"]["reason"]
            }
            csv_data.append(row)
        save_csv_summary(csv_data, f"{safe_filename(artist_name)}_final_summary.csv", csv_fieldnames)

    return final_subreddits_for_artist

def process_artist_list(artists):
    """
    Processes a list of artists to find their fan subreddits.
    Saves a global run audit log and summary.
    """
    run_metadata = {
        "run_timestamp": get_current_timestamp(),
        "artists_processed": artists,
        "config_parameters": CONFIG_PARAMS,
        "results_summary_by_artist": {}
    }
    
    all_artist_subreddit_results = {}
    for artist in artists:
        found_subs = find_artist_subreddits(artist)
        all_artist_subreddit_results[artist] = found_subs
        
        # Populate results summary for the global run metadata
        run_metadata["results_summary_by_artist"][artist] = [
            {
                "display_name": sub["display_name"],
                "subscribers": sub["subscribers"],
                "confidence_score": sub["confidence_score"],
                "included_in_final_results": sub["final_decision"]["included_in_final_results"]
            } for sub in found_subs
        ]

    # Save a comprehensive run audit log (JSON)
    save_json_audit(run_metadata, f"run_audit_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")

    return all_artist_subreddit_results

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    artists_to_check = [
        "Taylor Swift",
        "Kendrick Lamar",
        "Daft Punk",
        "Queen",
        "Lord Huron", 
        "Billie Eilish",
        "Led Zeppelin", 
        "Vulfpeck",
        "Car Seat Headrest",
        "The Weeknd"
    ]

    all_artist_subreddit_results = process_artist_list(artists_to_check)

    print("\n\n--- FINAL REPORT OF LIKELY FAN SUBREDDITS (Console Output) ---")
    for artist, subreddits in all_artist_subreddit_results.items():
        print(f"\nArtist: {artist}")
        if subreddits:
            for data in subreddits: 
                print(f"  r/{data['display_name']}")
                print(f"    - Subscribers: {data['subscribers']}")
                print(f"    - Avg Comments (Hot Posts): {data['activity'].get('avg_comments_on_hot_posts', 0):.1f}")
                print(f"    - Hottest Post Age: {data['activity'].get('hottest_post_age_hours', 9999):.1f} hours ago")
                print(f"    - Confidence Score: {data['confidence_score']:.2f} (Relevance: {data['relevance']['score']}, Activity: {data['activity']['score']})")
                print(f"    - Final Decision: {data['final_decision']['included_in_final_results']} - {data['final_decision']['reason']}")
                if data['public_description']:
                    print(f"    - Description: {data['public_description'][:150]}...") 
                else:
                    print("    - No description.")
        else:
            print("  No highly relevant or active fan subreddits found.")